# nnsight v0.6.0 Release Notes

## ðŸš€ Highlights

### Seamless Custom Code on NDIF

The biggest change in this release is a new serialization format that enables your local packages to work seamlessly on NDIFâ€”even if they aren't installed on the server.

**How it works:** nnsight now serializes functions and classes *by value* (source code) rather than by reference. This means your custom analysis functions and modules are sent along with your request and rebuilt on the server.

```python
from nnsight import ndif, LanguageModel
import mymodule

# Register your module for remote execution
ndif.register(mymodule)

model = LanguageModel("meta-llama/Llama-3.1-70B")

with model.trace("Hello world", remote=True):
    # Your local function works on NDIF!
    result = mymodule.my_analysis_function(model).save()
```

**What gets auto-registered:** Any classes or functions defined in your main script or in your working directory (anything not in `site-packages`) are automatically registered. You only need to call `ndif.register()` for external local packages.

**Python version flexibility:** The new serialization format enables Python 3.9+ clients to work with NDIF regardless of the server's Python version.

---

### Environment Comparison with `ndif.compare()`

New utility to compare your local environment with NDIF's remote environment:

```python
from nnsight import ndif

ndif.compare()
```

This displays a table highlighting package version discrepancies between your local setup and NDIF, with critical packages (nnsight, transformers, torch) flagged prominently.

---

### Smarter Trace vs Invoker Detection

Previously, nnsight determined whether to use invokers by checking if you passed positional arguments to `.trace()`:

```python
# Old behavior:
with model.trace("Hello world"):  # Pos arg â†’ single invoker
    ...

with model.trace() as tracer:  # No pos args â†’ wait for sub-invokers
    with tracer.invoke("hello"):
        ...
```

**The problem:** If you only passed keyword arguments, nnsight would incorrectly wait for sub-invokers:

```python
# This would ERROR in older versions:
with model.trace(input_ids=my_ids):
    output = model.output.save()  # "can't access .output while not interleaving"
```

**Now:** The batching logic detects whether arguments affect the batch size. Arguments like `input_ids` are recognized as "real" inputs and create an implicit invoker. Configuration-only kwargs (like `max_new_tokens`) correctly allow sub-invokers:

```python
# Now works correctly:
with model.trace(input_ids=my_ids):
    hidden = model.transformer.h[0].output[0].save()

# Still supports invokers for generation config:
with model.generate(max_new_tokens=10) as tracer:
    with tracer.invoke("hello"):
        ...
    with tracer.invoke("goodbye"):
        ...
```

---

### Cleaner Exception Tracebacks

Exceptions inside traces now show clean, readable stack traces without nnsight internals cluttering the output:

```
Traceback (most recent call last):
  File "my_script.py", line 9, in <module>
    with model.trace("Hello world"):
  File "my_script.py", line 11, in <module>
    output = model.transformer.h[999].output.save()

IndexError: list index out of range
```

For debugging nnsight internals, run your script with the `-d` or `--debug` flag to include the full stack trace:

```bash
python -d my_script.py
```

---

### For-Loop Iteration

`tracer.iter` now supports standard Python `for` loops as an alternative to `with` blocks. The `for` loop version is faster because it avoids the source extraction, AST parsing, and compilation that the `with` block requiresâ€”the loop body runs inline in the worker thread.

```python
# Preferred: for-loop (faster, no code capture overhead)
with model.generate("Hello", max_new_tokens=5) as tracer:
    logits = list().save()
    for step in tracer.iter[:]:
        logits.append(model.lm_head.output[0][-1].argmax(dim=-1))

# Bounded slices and step index work the same way
with model.generate("Hello", max_new_tokens=5) as tracer:
    hs = list().save()
    for idx in tracer.iter[:3]:
        hs.append(model.transformer.h[idx].output[0])

# Conditionals per step
with model.generate("Hello", max_new_tokens=5) as tracer:
    for step in tracer.iter[:]:
        if step == 2:
            model.transformer.h[0].output[0][:] = 0
```

The `with` block version still works and behaves identically:

```python
# Still supported
with model.generate("Hello", max_new_tokens=5) as tracer:
    logits = list().save()
    with tracer.iter[:]:
        logits.append(model.lm_head.output[0][-1].argmax(dim=-1))
```

All iteration types are supported: unbounded (`iter[:]`), bounded slices (`iter[1:3]`), single indices (`iter[0]`), and lists (`iter[[0, 2, 4]]`).

---

## Other Changes

- **`python -c` support:** nnsight now works when running code via `python -c "..."`. Previously, this would fail because nnsight couldn't capture the source code. Now you can run quick experiments directly from the command line:

  ```bash
  python -c "
  from nnsight import LanguageModel
  model = LanguageModel('openai-community/gpt2')
  with model.trace('Hello world'):
      hidden = model.transformer.h[0].output[0].save()
  print(hidden.shape)
  "
  ```

- **Multiple NNsight wrappers on the same model:** You can now wrap the same PyTorch model with multiple `NNsight` or `LanguageModel` instances without breaking tracing. Previously, wrapping the same model twice would remove the first wrapper's hooks. Now hooks from multiple wrappers coexist, and each wrapper remains fully functional:

  ```python
  from nnsight import NNsight
  
  model1 = NNsight(my_pytorch_model)
  model2 = NNsight(my_pytorch_model)  # Same underlying model
  
  # Both wrappers work independently
  with model1.trace(input):
      out1 = model1.layer.output.save()
  
  with model2.trace(input):
      out2 = model2.layer.output.save()
  
  # model1 still works after model2 wrapped the same model
  with model1.trace(input):
      out3 = model1.layer.output.save()
  ```

- **Memory leak fixes:** Fixed reference loops that could cause memory leaks when wrapping models. The interleaver now uses weak references to avoid circular references, and hooks are properly cleaned up when the wrapper is garbage collected.

- **vLLM 0.14.1 support:** Updated vLLM integration for compatibility with vLLM 0.14.1
- **Compressed results:** Results from NDIF are now compressed with zstandard for smaller download sizes and faster transfers
---

### Performance: 2.4-3.9x Faster Traces

Trace overhead has been dramatically reduced through a series of optimizations targeting both the trace setup path and the per-hook execution path. On a 12-layer MLP benchmark (CPU), nnsight traces are **2.4-3.9x faster** than v0.5.15:

![v0.5.15 vs v0.6.0](performance/plots/version_comparison.png)

| Scenario | v0.5.15 | v0.6.0 | Speedup |
|----------|---------|---------|---------|
| Empty trace (no saves) | 1,196 us | 308 us | **3.9x** |
| 1 `.save()` call | 1,370 us | 474 us | **2.9x** |
| 12 `.save()` calls | 1,697 us | 716 us | **2.4x** |

The fixed setup cost (source extraction, AST parsing, compilation, thread creation) dropped from ~1,100 us to ~210 us. The marginal cost per `.save()` dropped from ~42 us to ~34 us.

![Overhead breakdown](performance/plots/overhead_breakdown.png)

**Where the savings come from:**

- **Always-on trace caching:** Source lines, AST nodes, and compiled code objects are now cached automatically. The first trace at a given call site pays the full compilation cost; subsequent calls skip source extraction, AST parsing, and compilation entirely. The `TRACE_CACHING` config is deprecated (caching is now always enabled).
- **Persistent pymount:** The `.save()` and `.stop()` methods are now mounted once at import and never unmounted, eliminating expensive `PyType_Modified()` calls that previously invalidated all Python type caches on every trace enter/exit.
- **Removed `torch._dynamo.disable` wrappers:** The `@torch._dynamo.disable` decorator on hook functions added unnecessary `set_eval_frame` C calls on every module forward. Since the hooks use closures, thread synchronization, and mutable state that dynamo can't compile anyway, removing the decorator is safe and saves ~4 C calls per hook invocation.
- **Batched `PyFrame_LocalsToFast`:** Cross-invoker variable sharing previously called the CPython `PyFrame_LocalsToFast` C API once per variable. Now all variables are batched into a single dict update and synced with one call.
- **Filtered globals copy:** When starting an intervention thread, nnsight previously copied the entire module `__globals__` dict. Now it only copies the global names actually referenced by the intervention's bytecode (`co_names`).

**How nnsight overhead compares to bare PyTorch hooks:**

![Scaling with saves](performance/plots/scaling.png)

The remaining overhead beyond raw PyTorch hooks comes from nnsight's feature set: thread-based deferred execution, source code extraction, automatic batching, cross-invoke variable sharing, and the event-driven mediator protocol. This overhead is constant regardless of model size, so it becomes negligible for real models where the forward pass dominates.

![Overhead vs hooks](performance/plots/overhead_vs_hooks.png)

**Other performance-related changes:**
- **`.save()` outside trace raises error:** Since pymount is now persistent, calling `.save()` outside a trace context now raises a `RuntimeError` with a clear message instead of silently succeeding.
- **Cache clearing:** `Globals.cache.clear()` is available to clear all cached source, AST, and code objects if needed.

---

<details>
<summary><b>Advanced: Local Serialization Testing</b></summary>

You can test if your code will serialize correctly for NDIF without actually submitting to the remote server using `remote='local'`:

```python
from nnsight import LanguageModel

model = LanguageModel("openai-community/gpt2")

# Simulates NDIF serialization locally
with model.trace("Hello world", remote='local'):
    hidden = model.transformer.h[0].output[0].save()

print("Success! Your code will work on NDIF.")
```

This is useful for catching serialization issues before submitting expensive remote jobs.

</details>

---

## Breaking Changes

If you have custom model classes that implement their own `_prepare_input` or `_batch` methods, you may need to update them to match the new signature. See the `Batchable` class in `nnsight/intervention/batching.py` for the updated interface.
